{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Measuring the Impact of News Headlines on Stock Movements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yfinance as yf\n",
    "from useful.eda import basic_info\n",
    "\n",
    "FAT_BAR = '='*50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Abstract\n",
    "\n",
    "The Dow Jones Industrial Average (DJIA) measures the performance of 30 companies (https://money.cnn.com/data/dow30/) and some consider it to be an indicator of the American economy. In general the market price of the index is a reflection of investors attitudes about it's future direction. If the market is rising and expected to rise then we call this a bull market, and in the opposite situation we call it a bear market.  \n",
    "\n",
    "There have been many attempts to measure investor sentiment as it could be indicative of the market's direction and thus assist in trading strategies which consist of fundamentally buying or selling. Although the market behaves randomly there may be ways to predict market movements in special situations. \n",
    "\n",
    "Given that the market reacts to investors attitudes we can speculate that the news either  has an effect on market's price or that it moves the market itself. We cannot be certain which of the two if any, causes market movements and we must be very careful not to say that whenever headline A happens market movement B will happen. However, my suspicision is that under certain market circumstances we can make better guesses at what will happen.\n",
    "\n",
    "The goal of this notebook is to train a supervised model to predict whether the market will go up or down over a certain period. To accomplish this we will take a set of headlines from 2008 to 2015 and match it with the closing price of the Down Jones Industrial Average from that same time period. We will use a natural language technique called bag of words, expressed through sci-kit learn's count-vectorize library to create the feature set for this model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The Data\n",
    "\n",
    "The stock data comes from yfiance (https://pypi.org/project/yfinance/), and we are using the DJIA index. The DJIA index consists of Open, High, Low, Close, and Volume. For our purposes we will use Close as it is considered the standard benchmark for performance overtime (https://www.investopedia.com/terms/c/closingprice.asp). Stock data are discrete random variables and the index goes out to two decimal places. The data source I have obtained only goes out to one decimal place. It should be noted that the actual return on the stock is considered a continuous random variable because it can have an infinite number of possible returns in theory.\n",
    "\n",
    "The news data set comes from reddit news headlines. It is worthy noting that the news data has multiple different headlines for the same day, as opposed to the stock data which has only one closing price. The news headlines contain strings of the headlines and will be transfomed into vectors in which each row will be a news headlines and each column will be a feature (word or bi-gram). \n",
    "\n",
    "The data will be converted to feather format from csv. For more info on the feather format please visit: https://github.com/wesm/feather. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Import the data and convert to feather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get data file directory\n",
    "DATA_DIR = os.path.join(str(Path.cwd().parent)+'/data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unzip only csv file for reddit headlines\n",
    "#TODO - if file is present unzip else leave\n",
    "for f in [f for f in os.listdir(DATA_DIR) if f.endswith('.zip')]:\n",
    "    ! cd .. && unzip data/{f} -d data/ && cd notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert csv --> feather\n",
    "for f in [f for f in os.listdir(DATA_DIR) if f.endswith('.csv')]:\n",
    "    print(f'loading {f}')\n",
    "    df = pd.read_csv(os.path.join(DATA_DIR, f))\n",
    "    df.to_feather(os.path.join(DATA_DIR, f\"{f.replace('.csv', '')}.feather\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = list(filter(lambda x: x.endswith('.feather'), os.listdir(DATA_DIR)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pull all the dfs into a dictionary for future use & print out basic data\n",
    "data = {i.split('.')[0]: pd.read_feather(\n",
    "            os.path.join(\n",
    "                DATA_DIR + '/' + [x for x in files if x.startswith(i.split('.')[0])][0]\n",
    "            )\n",
    "        ) for i in files}\n",
    "\n",
    "news_df = data['RedditNews']\n",
    "news_df = news_df.set_index('Date') \n",
    "\n",
    "news_df = news_df.iloc[::-1]\n",
    "\n",
    "basic_info.data_info(news_df,None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### News headlines\n",
    "\n",
    "As you can see above the news headlines data consists of the date and a column for news columns. The news column is an object (string) and the date is the index. If the date is in object format we can handle this later through pandas datetime. There is no missing data, and it looks like there are about 71 redudant headlines, with only one headline occuring 3 times. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### DJIA data\n",
    "\n",
    "The DJIA data is downloaded from the yfinance library and I set the dates from 2008-06-08 to 2016-07-01 to match with the news headlines. If there is some overlap this will be corrected when I merge the datasets.\n",
    "\n",
    "There are 1820 floats in 5 columns consisting of Open, High, Low, Close, Adj Close, and Volume. There is no missing data and no duplicates. \n",
    "\n",
    "The dates covered in this dataset are significant in that they cover the 2008 recission and post 2008 recovery. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dji = yf.download('DJI')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dji_df = dji[(dji.index >= '2008-06-08') & (dji.index <= '2016-07-01')]\n",
    "\n",
    "basic_info.data_info(dji_df,None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Graph of the DJIA closing price from 2008-06-08 to 2016-07-01\n",
    "\n",
    "This is a plot of the closing price from June 2008 to July 2016...\n",
    "\n",
    "### Convert to date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(20, 6))\n",
    "\n",
    "sns.tsplot(data=dji_df.loc[:,'Close'])\n",
    "plt.title('Dow Jones closing price from 2008-06-08 - 2016-07-01')\n",
    "plt.ylabel('Closing Price')\n",
    "plt.xlabel('Days')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dow Jones Feature engineering: Prediction Horizon\n",
    "\n",
    "At the heart of this project is to track the predictive ability of news headlines. To do so we need to see if a news headline effects the price of the DJIA at some point in the future. To begin this exericise I chose a 10 day horizon. All we are tracking for our variable is whether the stock went up or down on a n-day horizon. For example if we were to take a price on day 0 and measure it against day 10, if that price on day 10 were high than day one we would assign that day 0 date a 1 (up). If day 10 were lower than day 0, we assign the day 0 date a 0 (down). It is not clear what is the best prediction horizon we should use so I arbitrarily chose 10 days. I will use 5 days and 15 as well to see if we can obtain better results. To limit the scope of this notebook I will limit the prediction horizon to these days. \n",
    "\n",
    "##### Engineering the prediction horizon - further details...\n",
    "\n",
    "The goal is to match up a headline with a closing price n days in the future. The assumption being that the effects of the events underlying the headline will manifest itself n days in the future. For example if we use a 10 day prediction horizon the closing price in our shifed dataframe for 2008-06-09 will be from 2008-06-25 (10 days in the future based on the dates we have in our dataset). Therefore when we merge the stock price with the headline dataframe the algorithm will be trained on that day's headline and a price n days in the future. Our final step will be to assign a value (0 or 1) corresponding to whether the market goes up or down in our prediction time frame. Using our example above our final target variable will be a value that is 0 if the value on 2008-06-09 is greater than 2008-06-25 or 1 if the situation is reversed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Prediction horizon example\n",
    "\n",
    "Below I combined the original dataset side by side with the shifted values & the prediction value to illustrate my description above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare the data by shifting according to our desired horizon\n",
    "pred_df = dji_df.shift(-10)['Close'].iloc[:-10]\n",
    "\n",
    "example_df = pd.concat([dji_df['Close'].head(14), pred_df.head(14)],axis=1)\n",
    "example_df.columns = ['Close','Close-shift']\n",
    "\n",
    "example_df['Prediction'] = (example_df['Close-shift'] >= example_df['Close']).astype(int)\n",
    "\n",
    "example_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just to better illustrate our variable set we've taken the closing price from 2008-06-09 compared it to 2008-06-25 (the 10th value in our DJIA data) and returned a 0 because the 2008-06-25 data was less than the closing price for 2006-06-09. Essentially the price has dropped from day 0 as compared to day 10.\n",
    "\n",
    "##### Calculate the prediction variable \n",
    "\n",
    "Below we calculate the prediction variable based off the logic above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = [i for i in range(5,20) if i%5==0] #prediction horizons 5,10,15\n",
    "names = ['pred_df_'+ str(v) for v in values] #names of the values based off of the horizons\n",
    "\n",
    "pred_d = {name: \n",
    "             (dji_df.shift(-value)['Close'] >= dji_df['Close']).iloc[:-value].astype(int) \n",
    "             for name,value in zip(names,values)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#name the datasets\n",
    "pred_df_5 = pred_d['pred_df_5']\n",
    "pred_df_10 = pred_d['pred_df_10']\n",
    "pred_df_15 = pred_d['pred_df_15']\n",
    "\n",
    "print('5 day')\n",
    "print(pred_df_5.value_counts())\n",
    "print(FAT_BAR)\n",
    "print('10 day')\n",
    "print(pred_df_10.value_counts())\n",
    "print(FAT_BAR)\n",
    "print('15 day')\n",
    "print(pred_df_15.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Merge the DJIA data with the news headlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df.index = pd.to_datetime(news_df.index) #set news_df index to datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['combined_df_'+ str(v) for v in [i for i in range(5,20) if i%5==0]] \n",
    "\n",
    "combined = {name:\n",
    "        news_df.merge(pred_d[[k for k in pred_d.keys()][i]], left_on='Date' , right_on='Date', how='right')\n",
    "        for name, i in zip(names,range(len(names)))\n",
    "    }\n",
    "\n",
    "combined_df_5  = combined['combined_df_5']\n",
    "combined_df_10 = combined['combined_df_10']\n",
    "combined_df_15 = combined['combined_df_15']\n",
    "\n",
    "combined_df_10.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Bull/Bear datasets\n",
    "\n",
    "We may have more luck training the data on periods in time when the market is moving in a general trend. A bear market is characteristic of a downtrend and a bull is characterized as an upward trend. According to some economists the 2008 receission started in fall 2008 and ended in June 2009. Below I dilenate to dataframes \"bull\" and \"bear\" based on the dates above for use below. The idea being that the general market momentum in that discrete timeframe may produce higher prediction results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bull_names = ['bull_df_'+ str(v) for v in [i for i in range(5,20) if i%5==0]] \n",
    "bear_names = ['bear_df_'+ str(v) for v in [i for i in range(5,20) if i%5==0]] \n",
    "\n",
    "bull = {name:\n",
    "        combined[[k for k in combined.keys()][i]][combined[[k for k in combined.keys()][i]].index >= '2009-07-01']\n",
    "        for name, i in zip(bull_names,range(len(bull_names)))\n",
    "    }\n",
    "\n",
    "bear = {name:\n",
    "        combined[[k for k in combined.keys()][i]][combined[[k for k in combined.keys()][i]].index <= '2009-05-01']\n",
    "        for name, i in zip(bear_names,range(len(bear_names)))\n",
    "    }\n",
    "\n",
    "bull_df_5 = bull['bull_df_5']\n",
    "bull_df_10 = bull['bull_df_10']\n",
    "bull_df_15 = bull['bull_df_15']\n",
    "\n",
    "bear_df_5 = bear['bear_df_5']\n",
    "bear_df_10 = bear['bear_df_10']\n",
    "bear_df_15 = bear['bear_df_15']\n",
    "\n",
    "bear_df_5.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NLP feature engineering\n",
    "\n",
    "##### Extracting features from text\n",
    "\n",
    "To extract features for training the headlines we will use the bag of words model to train the headlines. Bag of words assigns a value to a word and counts the most frequent words in the dataset. Scikit learn provides a method of creating bag of words using countvectorizor, in which the columns are words and the rows are documents (headlines). \n",
    "\n",
    "Before we start though, we must clean up the text. This will consist of removing stopwords, making all the characters lowercase, stemming (finding the root: ask and asked), and converting to bigrams to help the algorithm predict what word is next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk import bigrams\n",
    "\n",
    "STEMMER = SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "\n",
    "def preprocess(text: pd.Series, *args):\n",
    "    text = text.apply(gensim.utils.simple_preprocess, min_len=3)\n",
    "    sw = set(stopwords.words('english'))\n",
    "\n",
    "    text = text.apply(lambda s: [w for w in s if w not in sw])\n",
    "    text = text.apply(lambda s: [STEMMER.stem(w) for w in s])\n",
    "    text = text.apply(lambda s: ['_'.join(x) for x in nltk.bigrams(s)] + s)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Preprocessing data...\n",
    "\n",
    "Below is a quick snippet of what our data will look like once it has been pre processed. As you can see the data has been evaluated with the steps outlined above. I've printed out the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bull_df_ = preprocess(bull_df.News)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bull_df_.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(analyzer = \"word\",   \n",
    "                             tokenizer = None,    \n",
    "                             preprocessor = None, \n",
    "                             stop_words = None,   \n",
    "                             max_features = 5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purposes of this notebook I chose 5000 features, and this was determined somewhat arbitrarily. I will determine empirically if I should grow/shrink the dataset after running my results.\n",
    "\n",
    "Below is an example of the cleaned news dataset..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bull_['bull_df_5_']['News'].apply(lambda x: ', '.join(map(str, x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply it to bull/bear datasets\n",
    "\n",
    "bull_ = {name + '_':\n",
    "     vectorizer.fit_transform(preprocess(bull[name].News).apply(lambda x: ', '.join(map(str, x))))\n",
    "     for name in bull.keys()\n",
    "    }\n",
    "\n",
    "bear_ = {name + '_':\n",
    "     vectorizer.fit_transform(preprocess(bear[name].News).apply(lambda x: ', '.join(map(str, x))))\n",
    "     for name in bear.keys()\n",
    "    }\n",
    "\n",
    "\n",
    "bull_['bull_df_5_']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Naive Bayes model\n",
    "\n",
    "First we will train on a Naive Bayes model\n",
    "\n",
    "For the bull data we check what our odds are for choosing a 0 or 1 at random our as that is our baseline comparison tool against our model. If our model were to perform as good or worse than probability then we would know that we are not seeing anything interesting. As you can see below for the bull data we have to do better than ~40% for downs and ~60-63% for ups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics as metrics\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "#roc plot\n",
    "\n",
    "def roc_plot(fpr, tpr, roc_auc):\n",
    "    # method I: plt\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "    plt.legend(loc = 'lower right')\n",
    "    plt.plot([0, 1], [0, 1],'r--')\n",
    "    plt.xlim([0, 1])\n",
    "    plt.ylim([0, 1])\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('odds of choosing a value at random')\n",
    "print('')\n",
    "print('horizon  down  up')\n",
    "\n",
    "for d in bull.keys():\n",
    "    x = bull[d]['Close'].value_counts()[0] + bull[d]['Close'].value_counts()[1]\n",
    "    zero,one = bull[d]['Close'].value_counts()[0]/x, bull[d]['Close'].value_counts()[1]/x\n",
    "    print(d, round(zero,2), round(one,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "clf = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d,i in zip(bull_.keys(),bull.keys()):               \n",
    "    X,y = bull_[d], bull[i]['Close']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 465)\n",
    "    clf.fit(X_train,y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    horizon = re.findall(\"[0-9]*$\",i[-2:])[0]\n",
    "    print(f'{d[:4]} {horizon} day prediction horizon')\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    probs = clf.predict_proba(X_test)\n",
    "    preds = probs[:,1]\n",
    "    fpr, tpr, threshold = metrics.roc_curve(y_test, preds)\n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "    roc_plot(fpr,tpr,roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('odds of choosing a value at random')\n",
    "print('')\n",
    "print('horizon  down  up')\n",
    "\n",
    "for d in bear.keys():\n",
    "    x = bear[d]['Close'].value_counts()[0] + bear[d]['Close'].value_counts()[1]\n",
    "    zero,one = bear[d]['Close'].value_counts()[0]/x, bear[d]['Close'].value_counts()[1]/x\n",
    "    print(d, round(zero,2), round(one,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d,i in zip(bear_.keys(),bear.keys()):               \n",
    "    X,y = bear_[d], bear[i]['Close']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 465)\n",
    "    clf.fit(X_train,y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    horizon = re.findall(\"[0-9]*$\",i[-2:])[0]\n",
    "    print(f'{d[:4]} {horizon} day prediction horizon')\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    probs = clf.predict_proba(X_test)\n",
    "    preds = probs[:,1]\n",
    "    fpr, tpr, threshold = metrics.roc_curve(y_test, preds)\n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "    roc_plot(fpr,tpr,roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import ensemble\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "rfc = ensemble.RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "# We'll make 500 iterations, use 2-deep trees, and set our loss function.\n",
    "params = {'n_estimators': 500,\n",
    "          'max_depth': 2,\n",
    "          'loss': 'deviance'}\n",
    "\n",
    "# Initialize and fit the model.\n",
    "ens = ensemble.GradientBoostingClassifier(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d,i in zip(bull_.keys(),bull.keys()):               \n",
    "    X,y = bull_[d], bull[i]['Close']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 465)\n",
    "    ens.fit(X_train,y_train)\n",
    "    y_pred = ens.predict(X_test)\n",
    "    horizon = re.findall(\"[0-9]*$\",i[-2:])[0]\n",
    "    print(f'{d[:4]} {horizon} day prediction horizon')\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    probs = ens.predict_proba(X_test)\n",
    "    preds = probs[:,1]\n",
    "    fpr, tpr, threshold = metrics.roc_curve(y_test, preds)\n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "    roc_plot(fpr,tpr,roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d,i in zip(bear_.keys(),bear.keys()):               \n",
    "    X,y = bear_[d], bear[i]['Close']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 465)\n",
    "    ens.fit(X_train,y_train)\n",
    "    y_pred = ens.predict(X_test)\n",
    "    horizon = re.findall(\"[0-9]*$\",i[-2:])[0]\n",
    "    print(f'{d[:4]} {horizon} day prediction horizon')\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    probs = ens.predict_proba(X_test)\n",
    "    preds = probs[:,1]\n",
    "    fpr, tpr, threshold = metrics.roc_curve(y_test, preds)\n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "    roc_plot(fpr,tpr,roc_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Small sample from 4Q 2010\n",
    "\n",
    "Here we take a small sample from the financial fourth quarter of 2010. As you can see from the graph below the market gained almost 1000 points during this time period. The motivation behind this is to see if we can predict better on a small period in time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 6))\n",
    "\n",
    "q = dji_df[(dji_df.index >= '2010-10-01') & (dji_df.index <= '2010-12-31')]['Close']\n",
    "\n",
    "sns.tsplot(data=q)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#establish the time period\n",
    "market_4q = dji_df[(dji_df.index >= '2010-10-01') & (dji_df.index <= '2010-12-31')] \n",
    "\n",
    "#names of the values based off of the horizons\n",
    "names = ['pred_df_'+ str(v) for v in [i for i in range(4,21)]] \n",
    "\n",
    "#create the shifted dataframes\n",
    "pred_4q = {name: \n",
    "             (market_4q.shift(-value)['Close'] >= market_4q['Close']).iloc[:-value].astype(int) \n",
    "             for name,value in zip(names,range(len(names)))\n",
    "        }\n",
    "\n",
    "#merge headlines with with shifted dfs\n",
    "combined = {name:\n",
    "            news_df.merge(pred_4q[[k for k in pred_4q.keys()][i]], left_on='Date' , right_on='Date', how='right')\n",
    "            for name, i in zip(names,range(len(names)))\n",
    "        }\n",
    "\n",
    "#NLP feature engineering\n",
    "names_ = ['pred_df_'+ str(v) for v in [i for i in range(5,21)]]\n",
    "\n",
    "combined_ = {name + '_':\n",
    "             vectorizer.fit_transform(preprocess(combined[name].News).apply(lambda x: ', '.join(map(str, x))))\n",
    "             for name in names_\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.utils import resample\n",
    "\n",
    "nvb = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d,i in zip(combined_.keys(),[d for d in combined.keys()][1:]):               \n",
    "    X,y = combined_[d], combined[i]['Close']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 465)\n",
    "    \n",
    "    y_t = pd.DataFrame(csr_matrix(y_train.values).toarray().reshape(-1,1))\n",
    "    y_t['Close'] = pd.DataFrame(csr_matrix(y_train.values).toarray().reshape(-1,1)) #transpose\n",
    "    X = pd.concat([pd.DataFrame(X_train.toarray()), y_t['Close']], axis=1) #convert from sparse matrix\n",
    "    \n",
    "    #upsample\n",
    "    down,up = X[X.Close==0],X[X.Close==1] \n",
    "    down_upsampled = resample(down,\n",
    "                              replace=True, # sample with replacement\n",
    "                              n_samples=len(up), # match number in majority class\n",
    "                              random_state=27) # reproducible results\n",
    "\n",
    "    # combine majority and upsampled minority\n",
    "    upsampled = pd.concat([down_upsampled, up])\n",
    "    \n",
    "    nvb.fit(upsampled.loc[:,~upsampled.columns.isin(['Close'])], upsampled['Close'])\n",
    "    y_pred = nvb.predict(X_test)\n",
    "    horizon = re.findall(\"[0-9]*$\",i[-2:])[0]\n",
    "    print(f'{d[:4]} {horizon} day prediction horizon')\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    probs = nvb.predict_proba(X_test)\n",
    "    preds = probs[:,1]\n",
    "    fpr, tpr, threshold = metrics.roc_curve(y_test, preds)\n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "    roc_plot(fpr,tpr,roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import ensemble\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "rfc = ensemble.RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "# We'll make 500 iterations, use 2-deep trees, and set our loss function.\n",
    "params = {'n_estimators': 500,\n",
    "          'max_depth': 2,\n",
    "          'loss': 'deviance'}\n",
    "\n",
    "# Initialize and fit the model.\n",
    "ens = ensemble.GradientBoostingClassifier(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d,i in zip(combined_.keys(),[d for d in combined.keys()][1:]):               \n",
    "    X,y = combined_[d], combined[i]['Close']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 465)\n",
    "    \n",
    "    y_t = pd.DataFrame(csr_matrix(y_train.values).toarray().reshape(-1,1))\n",
    "    y_t['Close'] = pd.DataFrame(csr_matrix(y_train.values).toarray().reshape(-1,1)) #transpose\n",
    "    X = pd.concat([pd.DataFrame(X_train.toarray()), y_t['Close']], axis=1) #convert from sparse matrix\n",
    "    \n",
    "    #upsample\n",
    "    down,up = X[X.Close==0],X[X.Close==1] \n",
    "    down_upsampled = resample(down,\n",
    "                              replace=True, # sample with replacement\n",
    "                              n_samples=len(up), # match number in majority class\n",
    "                              random_state=27) # reproducible results\n",
    "\n",
    "    # combine majority and upsampled minority\n",
    "    upsampled = pd.concat([down_upsampled, up])\n",
    "    \n",
    "    rfc.fit(upsampled.loc[:,~upsampled.columns.isin(['Close'])], upsampled['Close'])\n",
    "    y_pred = rfc.predict(X_test)\n",
    "    horizon = re.findall(\"[0-9]*$\",i[-2:])[0]\n",
    "    print(f'{d[:4]} {horizon} day prediction horizon')\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    probs = rfc.predict_proba(X_test)\n",
    "    preds = probs[:,1]\n",
    "    fpr, tpr, threshold = metrics.roc_curve(y_test, preds)\n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "    roc_plot(fpr,tpr,roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "svc = SVC(C=1.0, kernel='linear', class_weight='balanced', probability=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d,i in zip(combined_.keys(),[d for d in combined.keys()][1:]):               \n",
    "    X,y = combined_[d], combined[i]['Close']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 465)\n",
    "    svc.fit(X_train,y_train)\n",
    "    y_pred = svc.predict(X_test)\n",
    "    horizon = re.findall(\"[0-9]*$\",i[-2:])[0]\n",
    "    print(f'{d[:4]} {horizon} day prediction horizon')\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    probs = svc.predict_proba(X_test)\n",
    "    preds = probs[:,1]\n",
    "    fpr, tpr, threshold = metrics.roc_curve(y_test, preds)\n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "    roc_plot(fpr,tpr,roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf1 = SVC(C=1.0, kernel='linear', class_weight='balanced', probability=True)\n",
    "clf2 = GaussianNB()\n",
    "clf3 = RandomForestClassifier(n_estimators=50, random_state=1)\n",
    "\n",
    "eclf = VotingClassifier(estimators=[('svc', clf1), \n",
    "                                    ('gnb', clf2), \n",
    "                                    ('rfc', clf3)], \n",
    "                                    voting='soft')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d,i in zip(combined_.keys(),[d for d in combined.keys()][1:]):               \n",
    "    X,y = combined_[d].toarray(), combined[i]['Close']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 465)\n",
    "    eclf.fit(X_train,y_train)\n",
    "    y_pred = eclf.predict(X_test)\n",
    "    horizon = re.findall(\"[0-9]*$\",i[-2:])[0]\n",
    "    print(f'{d[:4]} {horizon} day prediction horizon')\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    probs = eclf.predict_proba(X_test)\n",
    "    preds = probs[:,1]\n",
    "    fpr, tpr, threshold = metrics.roc_curve(y_test, preds)\n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "    roc_plot(fpr,tpr,roc_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3Q 2008"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 6))\n",
    "\n",
    "q = dji_df[(dji_df.index >= '2008-07-01') & (dji_df.index <= '2008-10-30')]['Close']\n",
    "\n",
    "sns.tsplot(data=q)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#establish the time period\n",
    "market_3q = dji_df[(dji_df.index >= '2008-07-01') & (dji_df.index <= '2008-10-30')] \n",
    "\n",
    "#names of the values based off of the horizons\n",
    "names = ['pred_df_'+ str(v) for v in [i for i in range(4,21)]] \n",
    "\n",
    "#create the shifted dataframes\n",
    "pred_3q = {name: \n",
    "             (market_3q.shift(-value)['Close'] >= market_3q['Close']).iloc[:-value].astype(int) \n",
    "             for name,value in zip(names,range(len(names)))\n",
    "        }\n",
    "\n",
    "#merge headlines with with shifted dfs\n",
    "combined = {name:\n",
    "            news_df.merge(pred_3q[[k for k in pred_3q.keys()][i]], left_on='Date' , right_on='Date', how='right')\n",
    "            for name, i in zip(names,range(len(names)))\n",
    "        }\n",
    "\n",
    "#NLP feature engineering\n",
    "names_ = ['pred_df_'+ str(v) for v in [i for i in range(5,21)]]\n",
    "\n",
    "combined_ = {name + '_':\n",
    "             vectorizer.fit_transform(preprocess(combined[name].News).apply(lambda x: ', '.join(map(str, x))))\n",
    "             for name in names_\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.utils import resample\n",
    "\n",
    "nvb = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d,i in zip(combined_.keys(),[d for d in combined.keys()][1:]):               \n",
    "    X,y = combined_[d], combined[i]['Close']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 465)\n",
    "    \n",
    "    y_t = pd.DataFrame(csr_matrix(y_train.values).toarray().reshape(-1,1))\n",
    "    y_t['Close'] = pd.DataFrame(csr_matrix(y_train.values).toarray().reshape(-1,1)) #transpose\n",
    "    X = pd.concat([pd.DataFrame(X_train.toarray()), y_t['Close']], axis=1) #convert from sparse matrix\n",
    "    \n",
    "    #upsample\n",
    "    down,up = X[X.Close==0],X[X.Close==1] \n",
    "    down_upsampled = resample(down,\n",
    "                              replace=True, # sample with replacement\n",
    "                              n_samples=len(up), # match number in majority class\n",
    "                              random_state=27) # reproducible results\n",
    "\n",
    "    # combine majority and upsampled minority\n",
    "    upsampled = pd.concat([down_upsampled, up])\n",
    "    \n",
    "    nvb.fit(upsampled.loc[:,~upsampled.columns.isin(['Close'])], upsampled['Close'])\n",
    "    y_pred = nvb.predict(X_test)\n",
    "    horizon = re.findall(\"[0-9]*$\",i[-2:])[0]\n",
    "    print(f'{d[:4]} {horizon} day prediction horizon')\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    probs = nvb.predict_proba(X_test)\n",
    "    preds = probs[:,1]\n",
    "    fpr, tpr, threshold = metrics.roc_curve(y_test, preds)\n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "    roc_plot(fpr,tpr,roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d,i in zip(combined_.keys(),[d for d in combined.keys()][1:]):               \n",
    "    X,y = combined_[d], combined[i]['Close']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 465)\n",
    "    \n",
    "    y_t = pd.DataFrame(csr_matrix(y_train.values).toarray().reshape(-1,1))\n",
    "    y_t['Close'] = pd.DataFrame(csr_matrix(y_train.values).toarray().reshape(-1,1)) #transpose\n",
    "    X = pd.concat([pd.DataFrame(X_train.toarray()), y_t['Close']], axis=1) #convert from sparse matrix\n",
    "    \n",
    "    #upsample\n",
    "    down,up = X[X.Close==0],X[X.Close==1] \n",
    "    down_upsampled = resample(down,\n",
    "                              replace=True, # sample with replacement\n",
    "                              n_samples=len(up), # match number in majority class\n",
    "                              random_state=27) # reproducible results\n",
    "\n",
    "    # combine majority and upsampled minority\n",
    "    upsampled = pd.concat([down_upsampled, up])\n",
    "    \n",
    "    rfc.fit(upsampled.loc[:,~upsampled.columns.isin(['Close'])], upsampled['Close'])\n",
    "    y_pred = rfc.predict(X_test)\n",
    "    horizon = re.findall(\"[0-9]*$\",i[-2:])[0]\n",
    "    print(f'{d[:4]} {horizon} day prediction horizon')\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    probs = rfc.predict_proba(X_test)\n",
    "    preds = probs[:,1]\n",
    "    fpr, tpr, threshold = metrics.roc_curve(y_test, preds)\n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "    roc_plot(fpr,tpr,roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf1 = SVC(C=1.0, kernel='linear', class_weight='balanced', probability=True)\n",
    "clf2 = GaussianNB()\n",
    "clf3 = RandomForestClassifier(n_estimators=50, random_state=1)\n",
    "\n",
    "eclf = VotingClassifier(estimators=[('svc', clf1), \n",
    "                                    ('gnb', clf2), \n",
    "                                    ('rfc', clf3)], \n",
    "                                    voting='soft')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d,i in zip(combined_.keys(),[d for d in combined.keys()][1:]):               \n",
    "    X,y = combined_[d].toarray(), combined[i]['Close']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 465)\n",
    "    eclf.fit(X_train,y_train)\n",
    "    y_pred = eclf.predict(X_test)\n",
    "    horizon = re.findall(\"[0-9]*$\",i[-2:])[0]\n",
    "    print(f'{d[:4]} {horizon} day prediction horizon')\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    probs = eclf.predict_proba(X_test)\n",
    "    preds = probs[:,1]\n",
    "    fpr, tpr, threshold = metrics.roc_curve(y_test, preds)\n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "    roc_plot(fpr,tpr,roc_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Combine 4Q & 3Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#establish the time period\n",
    "market_3q = dji_df[(dji_df.index >= '2008-07-01') & (dji_df.index <= '2008-10-30')] \n",
    "market_4q = dji_df[(dji_df.index >= '2010-10-01') & (dji_df.index <= '2010-12-31')] \n",
    "\n",
    "df = pd.concat([market_3q,market_4q],axis=0)\n",
    "\n",
    "#names of the values based off of the horizons\n",
    "names = ['pred_df_'+ str(v) for v in [i for i in range(4,21)]] \n",
    "\n",
    "#create the shifted dataframes\n",
    "pred_df = {name: \n",
    "             (df.shift(-value)['Close'] >= df['Close']).iloc[:-value].astype(int) \n",
    "             for name,value in zip(names,range(len(names)))\n",
    "        }\n",
    "\n",
    "#merge headlines with with shifted dfs\n",
    "combined = {name:\n",
    "            news_df.merge(pred_df[[k for k in pred_df.keys()][i]], left_on='Date' , right_on='Date', how='right')\n",
    "            for name, i in zip(names,range(len(names)))\n",
    "        }\n",
    "\n",
    "#NLP feature engineering\n",
    "names_ = ['pred_df_'+ str(v) for v in [i for i in range(5,21)]]\n",
    "\n",
    "combined_ = {name + '_':\n",
    "             vectorizer.fit_transform(preprocess(combined[name].News).apply(lambda x: ', '.join(map(str, x))))\n",
    "             for name in names_\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf1 = SVC(C=1.0, kernel='linear', class_weight='balanced', probability=True)\n",
    "clf2 = GaussianNB()\n",
    "clf3 = RandomForestClassifier(n_estimators=50, random_state=1)\n",
    "\n",
    "eclf = VotingClassifier(estimators=[('svc', clf1), \n",
    "                                    ('gnb', clf2), \n",
    "                                    ('rfc', clf3)], \n",
    "                                    voting='soft')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d,i in zip(combined_.keys(),[d for d in combined.keys()][1:]):               \n",
    "    X,y = combined_[d].toarray(), combined[i]['Close']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 465)\n",
    "    eclf.fit(X_train,y_train)\n",
    "    y_pred = eclf.predict(X_test)\n",
    "    horizon = re.findall(\"[0-9]*$\",i[-2:])[0]\n",
    "    print(f'{d[:4]} {horizon} day prediction horizon')\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    probs = eclf.predict_proba(X_test)\n",
    "    preds = probs[:,1]\n",
    "    fpr, tpr, threshold = metrics.roc_curve(y_test, preds)\n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "    roc_plot(fpr,tpr,roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('odds of choosing a value at random')\n",
    "print('')\n",
    "print('horizon  down  up')\n",
    "\n",
    "for d in combined.keys():\n",
    "    x = bear[d]['Close'].value_counts()[0] + bear[d]['Close'].value_counts()[1]\n",
    "    zero,one = bear[d]['Close'].value_counts()[0]/x, bear[d]['Close'].value_counts()[1]/x\n",
    "    print(d, round(zero,2), round(one,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Conclusion and warning about extreme events\n",
    "\n",
    "### Precision v. recall tradeoff \n",
    "\n",
    "We appeared to have our best success on a small timeframe (4Q 2010) with prediction of upward movements on a 15 day horizon with an ensamble model depth of 2 and 500 iterations. Possible explanations could be that positive momentum leads to overall market optimism and further investment in the market (it's never going to go down!). Why a 15-day horizon produces slightly better results will warrent further investigation.\n",
    "\n",
    "Fundamentally we know that during a bull market we have more buyers than sellers thus the price of an asset will go up. In most cases (the rational case) you will buy an asset if you expect the price of that asset to gain value over time. As many famous investors have observed that the market can be a general evaluation of investor sentiment, we can speculate that optimism dominates during bull markets and the opposite during bear markets. The goal of this exercise was to see if we could capture that sentiment expressed through headlines.\n",
    "\n",
    "Ultimately the movement of a stock is a stochastic process and the economic change to a recession (where is the top v. bottom of a price) is something that this model lacks. We cannot say with certainty when the market will take a nosedive. Thus a more sophisticated model would consider not only predicted price movements but also magnitude of downside risk. Although our model predicted decently for 2010 Q4, the potential to wipe out small incremental gains over the course of many days in one day exists (See the Turkey problem/problem of induction). Additionally, this illustrates why we can't normalize the stock data. Predictions of 95% confidence of market moves may appear to be very favorable but tail events can have effects on your financial position.\n",
    "\n",
    "##### Future work\n",
    "\n",
    "Further analysis can be done on features have a greater effect during bull and bear markets. We can accomplish this through other methods of extracting word features such as TF-IDF and word2vec for example. Additionally we can applying other methods of classification the data such as neural networks. Finally we can generate what quant's call technical indicators to assist in the training of the models. There are numerous technical indicators that seek to measure price momentum.\n",
    "\n",
    "Another point that must be investigated is how can we predict market movements that go agains the macro-economic environment, such as down movements during a bull market and vice-versa. Our model did well predicting the market was going up during a bull market, but how much was this due to chance.\n",
    "\n",
    "Given more time, further investigations can be done into the optimal prediction horizon using a loop to create a dataframe for a given period of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
